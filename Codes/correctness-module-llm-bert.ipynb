{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, Automodel, AutoConfig # To tokenize the dataset for LLM consumption\n",
    "import torch.nn as nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectnessModuleLLM(nn.Module):\n",
    "    def __init__(self: Type[\"CorrectnessModuleLLM\"],\n",
    "                 checkpoint: str) -> None:\n",
    "        super(CorrectnessModuleLLM, self).__init()\n",
    "        self.embedding_body = Automodel.from_pretrained(checkpoint, \n",
    "                                                        config=AutoConfig.from_pretrained(checkpoint,\n",
    "                                                                                          output_attention=True,\n",
    "                                                                                          output_hidden_states=True))\n",
    "        self.logit_transform = nn.Linear(in_features = 768, # This should be somehow dynamic. Can be with the help of above model's config variable\n",
    "                                         out_features = 1,\n",
    "                                         bias=True)\n",
    "        self.output_transform = nn.Sigmoid()\n",
    "        return\n",
    "    def forward(self: Type[\"CorrectnessModuleLLM\"],\n",
    "                input_ids: Tensor,\n",
    "                attention_mask) -> Tensor:\n",
    "        llm_embeddings = self.embedding_body(input_ids=input_ids,\n",
    "                                             attention_mask=attention_mask)\n",
    "        cls_token_output = llm_embeddings.hidden_states[0]\n",
    "        logits = self.logit_transform(cls_token_output)\n",
    "        output_prob = self.output_transform(logits)\n",
    "        return output_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer code\n",
    "checkpoint_model_tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT, return_tensors='pt')\n",
    "\n",
    "def tokenize_and_align_dataset(sample):\n",
    "    question_string = sample['Question']\n",
    "    answer_string = sample[\"Answer\"]\n",
    "    tokenized_question = checkpoint_model_tokenizer(question_string, is_split_into_words=True)\n",
    "    tokenized_answer = checkpoint_model_tokenizer(answer_string, is_split_into_words=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
