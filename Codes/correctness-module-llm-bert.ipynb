{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, Automodel, AutoConfig # To tokenize the dataset for LLM consumption\n",
    "import pytorch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCorrectnessModule(nn.Module):\n",
    "    def __init__(self: Type[\"LLMCorrectnessModule\"],\n",
    "                 checkpoint: str) -> None:\n",
    "        super(LLMCorrectnessModule, self).__init()\n",
    "        self.embedding_body = Automodel.from_pretrained(checkpoint, \n",
    "                                                        config=AutoConfig.from_pretrained(checkpoint,\n",
    "                                                                                          output_attention=True,\n",
    "                                                                                          output_hidden_states=True))\n",
    "        self.logit_transform = nn.Linear(in_features = 768, # This should be somehow dynamic. Can be with the help of above model's config variable\n",
    "                                         out_features = 1,\n",
    "                                         bias=True)\n",
    "        self.output_transform = nn.Sigmoid()\n",
    "        return\n",
    "    def forward(self: Type[\"LLMCorrectnessModule\"],\n",
    "                input_ids,\n",
    "                attention_mask) -> FloatTensor:\n",
    "        llm_embeddings = self.embedding_body(input_ids=input_ids,\n",
    "                                             attention_mask=attention_mask)\n",
    "        cls_token_output = llm_embeddings.hidden_states[0]\n",
    "        logits = self.logit_transform(cls_token_output)\n",
    "        output_prob = self.output_transform(logits)\n",
    "        return output_prob\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
