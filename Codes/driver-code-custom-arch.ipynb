{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_from_disk\n",
    "import evaluate\n",
    "from ques_gen_chatgpt import generate_follow_up_question\n",
    "from user_agent_simulation_chatgpt import simulate_user_response\n",
    "import torch.nn as nn\n",
    "from typing import Type\n",
    "from torch import Tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectnessModuleLLM(nn.Module):\n",
    "    def __init__(self: Type[\"CorrectnessModuleLLM\"],\n",
    "                 checkpoint: str) -> None:\n",
    "        super(CorrectnessModuleLLM, self).__init__()\n",
    "        bert_config = AutoConfig.from_pretrained(checkpoint, output_attention=True, output_hidden_states=False)\n",
    "        self.embedding_body = AutoModel.from_pretrained(checkpoint, \n",
    "                                                        config=bert_config)\n",
    "        self.dense_hidden_1 = nn.Linear(in_features=bert_config.hidden_size, \n",
    "                                        out_features=512,\n",
    "                                        bias=True)\n",
    "        self.dense_hidden_activation_1 = nn.Tanh()\n",
    "        self.dense_hidden_2 = nn.Linear(in_features=512, \n",
    "                                        out_features=64,\n",
    "                                        bias=True)\n",
    "        self.dense_hidden_activation_2 = nn.ReLU()\n",
    "        self.logit_transform = nn.Linear(in_features=64, \n",
    "                                         out_features=1,\n",
    "                                         bias=True)\n",
    "        return\n",
    "    \n",
    "    def forward(self: Type[\"CorrectnessModuleLLM\"],\n",
    "                input_ids: Tensor,\n",
    "                attention_mask: Tensor,\n",
    "                token_type_ids) -> Tensor:\n",
    "        llm_embeddings = self.embedding_body(input_ids=input_ids,\n",
    "                                             attention_mask=attention_mask,\n",
    "                                             token_type_ids=token_type_ids)\n",
    "        cls_token_output = llm_embeddings.last_hidden_state[:, 0, :]\n",
    "        dense_intermediate_1 = self.dense_hidden_1(cls_token_output)\n",
    "        dense_intermediate_activated_1 = self.dense_hidden_activation_1(dense_intermediate_1)\n",
    "        dense_intermediate_2 = self.dense_hidden_2(dense_intermediate_activated_1)\n",
    "        dense_intermediate_activated_2 = self.dense_hidden_activation_2(dense_intermediate_2)\n",
    "        logits = self.logit_transform(dense_intermediate_activated_2)\n",
    "        return logits\n",
    "\n",
    "    def save_pretrained(self, save_directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the model's base transformer and custom layers.\n",
    "        \"\"\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        # Save the transformer part\n",
    "        self.embedding_body.save_pretrained(save_directory)\n",
    "        # Save the custom layers\n",
    "        torch.save(self.dense_hidden_1.state_dict(), f\"{save_directory}/dense_hidden_1.pt\")\n",
    "        torch.save(self.dense_hidden_2.state_dict(), f\"{save_directory}/dense_hidden_2.pt\")\n",
    "        torch.save(self.logit_transform.state_dict(), f\"{save_directory}/logit_transform.pt\")\n",
    "        # Save the activations (optional, if needed for any configuration metadata)\n",
    "        # Save any additional model configuration (if applicable)\n",
    "        config = {\"hidden_activation_1\": \"tanh\", \"hidden_activation_1\": \"ReLU\", \"output_activation\": \"relu\"}  \n",
    "        with open(f\"{save_directory}/custom_config.json\", \"w\") as f:\n",
    "            json.dump(config, f)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: str, *model_args, **kwargs) -> Type[\"CorrectnessModuleLLM\"]:\n",
    "        \"\"\"\n",
    "        Load the model's base transformer and custom layers.\n",
    "        \"\"\"\n",
    "        # Load transformer configuration\n",
    "        config = AutoConfig.from_pretrained(save_directory)\n",
    "        # Initialize the model\n",
    "        model = cls(checkpoint=save_directory)\n",
    "        # Load transformer weights\n",
    "        model.embedding_body = AutoModel.from_pretrained(save_directory, config=config)\n",
    "        # Load custom layer weights\n",
    "        model.dense_hidden_1.load_state_dict(torch.load(f\"{save_directory}/dense_hidden_1.pt\"))\n",
    "        model.dense_hidden_2.load_state_dict(torch.load(f\"{save_directory}/dense_hidden_2.pt\"))\n",
    "        model.logit_transform.load_state_dict(torch.load(f\"{save_directory}/logit_transform.pt\"))\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT = \"../Model_Checkpoints/closed-generative-qa/checkpoint-156\" # Don't forget to replace this with the best checkpoint\n",
    "QA_CHECKPOINT = \"../Model_Checkpoints/closed-generative-qa-FLAN-T5/checkpoint-390\"\n",
    "CORRECTNESS_MODULE_CHECKPOINT = \"../Model_Checkpoints/correctness-module-checkpoints-bert-uncased/bert-correctness-batch_32_lr_cosine_5e6_20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa_qa_testing = load_from_disk(\"../Datasets/Visa_QA_V4/\")[\"test\"].select(range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model_tokenizer = T5Tokenizer.from_pretrained(QA_CHECKPOINT, return_tensors='pt')\n",
    "correctness_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = T5ForConditionalGeneration.from_pretrained(QA_CHECKPOINT)\n",
    "correctness_model = CorrectnessModuleLLM(checkpoint=CORRECTNESS_MODULE_CHECKPOINT).from_pretrained(CORRECTNESS_MODULE_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_model_answer(question, meta_tags, context=\"\"):\n",
    "    base_prompt = \"You are an expert in dealing with questions of immigration and international travel. Answer the following question with proper reasoning\"\n",
    "    if len(context) == 0:\n",
    "        prompt = f\"{base_prompt}. Use the keywords to get some hints about the answer: Keywords: {', '.join(meta_tags.split(','))} Question: {question}\"\n",
    "    else:\n",
    "        prompt = f\"{base_prompt}. Take hints from the provided context and give a complete personalized answer: Context: {context}, Question: {question}\"\n",
    "    encoded_input = qa_model_tokenizer(prompt, return_tensors='pt')\n",
    "    output = qa_model.generate(**encoded_input)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_level(question, answer):\n",
    "    question_string = sample['question']\n",
    "    answer_string = sample[\"answer\"]\n",
    "    final_input_string = f\"{question_string} [SEP] {answer_string}\"\n",
    "    tokenized_inputs = correctness_tokenizer(final_input_string, add_special_tokens=True)\n",
    "    output_score = correctness_model.forward(**tokenized_inputs).item()\n",
    "    return output_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS = 5\n",
    "CONFIDENCE_THRESHOLD = 25\n",
    "def generate_answer(row):\n",
    "    question = row['question']\n",
    "    tags = row['meta_tags']\n",
    "    init_ans = generate_qa_model_answer(question, tags)\n",
    "    answers_history = [init_ans]\n",
    "    confidence = get_confidence_level(question, init_ans)\n",
    "    steps = 0\n",
    "    while steps < MAX_STEPS and confidence < CONFIDENCE_THRESHOLD:\n",
    "        new_question = generate_follow_up_question(question, answers_history[-1])\n",
    "        followup_ans = simulate_user_response(new_question, question)\n",
    "        new_ans = generate_qa_model_answer(question, tags, answers_history[-1])\n",
    "        confidence = get_confidence_level(question, new_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = visa_qa_testing.map(generate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogue = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "predictions = prediction['Predicted Answer']\n",
    "ground_truth = prediction['answer']\n",
    "print(\"====== ROUGE SCORE ======\")\n",
    "print(rogue.compute(predictions=predictions, references = ground_truth))\n",
    "print(\"====== BLEU SCORE ======\")\n",
    "print(bleu.compute(predictions=predictions, references=ground_truth))\n",
    "\n",
    "prediction.save_to_disk(\"../Datasets/Visa_prediction_FineTuned_FLANT5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
