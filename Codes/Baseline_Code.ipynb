{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC1_ScdgGWI-",
        "outputId": "dbb93872-cfa0-4738-8373-58e8318e6185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading and preprocessing data...\n",
            "Loaded DataFrame columns: Index(['QScore', 'Post Link', 'Title', 'Tags', 'Question Body', 'Questioner',\n",
            "       'AScore', 'Answer Body', 'AnswerDate'],\n",
            "      dtype='object')\n",
            "Preprocessing questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 610.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing answers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 883.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size after preprocessing: 20 rows\n",
            "\n",
            "Preprocessing Statistics:\n",
            "Average question length: 523.75 characters\n",
            "Average answer length: 741.80 characters\n",
            "\n",
            "Dataset splits:\n",
            "Training set: 14 samples\n",
            "Validation set: 3 samples\n",
            "Test set: 3 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import html\n",
        "from bs4 import BeautifulSoup\n",
        "from html.parser import HTMLParser\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "class QABertModel(torch.nn.Module):\n",
        "    def __init__(self, model_name='distilbert-base-uncased'):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        # Use [CLS] token embedding as the sentence representation\n",
        "        return outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "\n",
        "class HTMLStripper(HTMLParser):\n",
        "    \"\"\"Custom HTML Parser for stripping HTML tags\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.reset()\n",
        "        self.strict = False\n",
        "        self.convert_charrefs = True\n",
        "        self.text = []\n",
        "\n",
        "    def handle_data(self, d):\n",
        "        self.text.append(d)\n",
        "\n",
        "    def get_data(self):\n",
        "        return ' '.join(self.text)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Comprehensive text preprocessing function\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to string and lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Decode HTML entities\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # Remove HTML tags using BeautifulSoup\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Additional HTML cleaning using custom stripper\n",
        "    stripper = HTMLStripper()\n",
        "    stripper.feed(text)\n",
        "    text = stripper.get_data()\n",
        "\n",
        "    # Normalize unicode characters\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "\n",
        "    # Remove special characters but keep basic punctuation\n",
        "    text = re.sub(r'[^a-z0-9\\s.,!?-]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # Fix common contractions\n",
        "    contractions = {\n",
        "        \"won't\": \"will not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"n't\": \" not\",\n",
        "        \"'re\": \" are\",\n",
        "        \"'s\": \" is\",\n",
        "        \"'d\": \" would\",\n",
        "        \"'ll\": \" will\",\n",
        "        \"'t\": \" not\",\n",
        "        \"'ve\": \" have\",\n",
        "        \"'m\": \" am\"\n",
        "    }\n",
        "\n",
        "    for contraction, expansion in contractions.items():\n",
        "        text = text.replace(contraction, expansion)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and preprocess the dataset\n",
        "    \"\"\"\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "\n",
        "    # Read the data\n",
        "    df = pd.read_csv(file_path, on_bad_lines='skip', quoting=2).iloc[0:20]\n",
        "    print(f\"Loaded DataFrame columns: {df.columns}\")\n",
        "\n",
        "    # Make sure question and answer columns exist\n",
        "    required_columns = ['Question Body', 'Answer Body']\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Missing required column: {col}\")\n",
        "\n",
        "    # Preprocess questions and answers\n",
        "    print(\"Preprocessing questions...\")\n",
        "    df['Question Body'] = df['Question Body'].progress_apply(preprocess_text)\n",
        "\n",
        "    print(\"Preprocessing answers...\")\n",
        "    df['Answer Body'] = df['Answer Body'].progress_apply(preprocess_text)\n",
        "\n",
        "    # Remove rows where question or answer is empty after preprocessing\n",
        "    df = df.dropna(subset=['Question Body', 'Answer Body'])\n",
        "    df = df[df['Question Body'].str.strip() != '']\n",
        "    df = df[df['Answer Body'].str.strip() != '']\n",
        "\n",
        "    print(f\"Dataset size after preprocessing: {len(df)} rows\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, device, epochs=3):\n",
        "    \"\"\"\n",
        "    Train model using cosine similarity loss between questions and answers\n",
        "    \"\"\"\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "    # Calculate total training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch + 1}/{epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_dataloader):\n",
        "            # Get question and answer inputs\n",
        "            question_input_ids = batch['question_input_ids'].to(device)\n",
        "            question_attention_mask = batch['question_attention_mask'].to(device)\n",
        "\n",
        "            answer_input_ids = batch['answer_input_ids'].to(device)\n",
        "            answer_attention_mask = batch['answer_attention_mask'].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Get embeddings for questions and answers\n",
        "            question_embeddings = model(\n",
        "                input_ids=question_input_ids,\n",
        "                attention_mask=question_attention_mask,\n",
        "            )\n",
        "\n",
        "            answer_embeddings = model(\n",
        "                input_ids=answer_input_ids,\n",
        "                attention_mask=answer_attention_mask,\n",
        "            )\n",
        "\n",
        "            # Calculate similarity loss\n",
        "            loss = cosine_similarity_loss(question_embeddings, answer_embeddings)\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(f'Average training loss: {avg_train_loss}')\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "\n",
        "        for batch in tqdm(val_dataloader):\n",
        "            question_input_ids = batch['question_input_ids'].to(device)\n",
        "            question_attention_mask = batch['question_attention_mask'].to(device)\n",
        "\n",
        "            answer_input_ids = batch['answer_input_ids'].to(device)\n",
        "            answer_attention_mask = batch['answer_attention_mask'].to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                question_embeddings = model(\n",
        "                    input_ids=question_input_ids,\n",
        "                    attention_mask=question_attention_mask,\n",
        "                )\n",
        "\n",
        "                answer_embeddings = model(\n",
        "                    input_ids=answer_input_ids,\n",
        "                    attention_mask=answer_attention_mask,\n",
        "                )\n",
        "\n",
        "                loss = cosine_similarity_loss(question_embeddings, answer_embeddings)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "        print(f'Average validation loss: {avg_val_loss}')\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_similarities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_dataloader):\n",
        "            question_input_ids = batch['question_input_ids'].to(device)\n",
        "            question_attention_mask = batch['question_attention_mask'].to(device)\n",
        "\n",
        "            answer_input_ids = batch['answer_input_ids'].to(device)\n",
        "            answer_attention_mask = batch['answer_attention_mask'].to(device)\n",
        "\n",
        "            question_embeddings = model(\n",
        "                input_ids=question_input_ids,\n",
        "                attention_mask=question_attention_mask,\n",
        "            )\n",
        "\n",
        "            answer_embeddings = model(\n",
        "                input_ids=answer_input_ids,\n",
        "                attention_mask=answer_attention_mask,\n",
        "            )\n",
        "\n",
        "            # Calculate similarities\n",
        "            q_norm = F.normalize(question_embeddings, p=2, dim=1)\n",
        "            a_norm = F.normalize(answer_embeddings, p=2, dim=1)\n",
        "            similarities = torch.sum(q_norm * a_norm, dim=1)\n",
        "            all_similarities.extend(similarities.cpu().numpy())\n",
        "\n",
        "    return {\n",
        "        'mean_similarity': np.mean(all_similarities),\n",
        "        'median_similarity': np.median(all_similarities),\n",
        "        'std_similarity': np.std(all_similarities)\n",
        "    }\n",
        "\n",
        "def cosine_similarity_loss(question_embeddings, answer_embeddings):\n",
        "    \"\"\"\n",
        "    Compute cosine similarity loss between question and answer embeddings\n",
        "    \"\"\"\n",
        "    # Normalize embeddings\n",
        "    question_embeddings_norm = F.normalize(question_embeddings, p=2, dim=1)\n",
        "    answer_embeddings_norm = F.normalize(answer_embeddings, p=2, dim=1)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity = torch.sum(question_embeddings_norm * answer_embeddings_norm, dim=1)\n",
        "\n",
        "    # Convert similarity to loss (1 - similarity to minimize)\n",
        "    loss = 1 - similarity.mean()\n",
        "\n",
        "    return loss\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, questions, answers, tokenizer, max_length=512):\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = str(self.questions[idx])\n",
        "        answer = str(self.answers[idx])\n",
        "\n",
        "        # Tokenize question and answer separately\n",
        "        question_encoding = self.tokenizer(\n",
        "            question,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        answer_encoding = self.tokenizer(\n",
        "            answer,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'question_input_ids': question_encoding['input_ids'].squeeze(),\n",
        "            'question_attention_mask': question_encoding['attention_mask'].squeeze(),\n",
        "            'answer_input_ids': answer_encoding['input_ids'].squeeze(),\n",
        "            'answer_attention_mask': answer_encoding['attention_mask'].squeeze(),\n",
        "        }\n",
        "\n",
        "# [Previous code for cosine_similarity_loss, QABertModel, train_model, and evaluate_model remains exactly the same]\n",
        "\n",
        "def save_evaluation_results(results, filename='evaluation_results.txt'):\n",
        "    \"\"\"\n",
        "    Save evaluation results to a file\n",
        "    \"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"Evaluation Results\\n\")\n",
        "        f.write(\"=================\\n\\n\")\n",
        "        for metric, value in results.items():\n",
        "            f.write(f\"{metric}: {value:.4f}\\n\")\n",
        "\n",
        "def main():\n",
        "    # Enable progress bar for pandas operations\n",
        "    tqdm.pandas()\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        # Load and preprocess data\n",
        "        df = load_and_preprocess_data(\"/content/VIsa_Questions_Stack_Exchange_V2.csv\")\n",
        "\n",
        "        # Print some preprocessing statistics\n",
        "        print(\"\\nPreprocessing Statistics:\")\n",
        "        print(f\"Average question length: {df['Question Body'].str.len().mean():.2f} characters\")\n",
        "        print(f\"Average answer length: {df['Answer Body'].str.len().mean():.2f} characters\")\n",
        "\n",
        "        # Split data into train, validation, and test sets\n",
        "        train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "        print(\"\\nDataset splits:\")\n",
        "        print(f\"Training set: {len(train_df)} samples\")\n",
        "        print(f\"Validation set: {len(val_df)} samples\")\n",
        "        print(f\"Test set: {len(test_df)} samples\")\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = QABertModel('bert-base-uncased')\n",
        "        model.to(device)\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = QADataset(\n",
        "            train_df['Question Body'].values,\n",
        "            train_df['Answer Body'].values,\n",
        "            tokenizer\n",
        "        )\n",
        "        val_dataset = QADataset(\n",
        "            val_df['Question Body'].values,\n",
        "            val_df['Answer Body'].values,\n",
        "            tokenizer\n",
        "        )\n",
        "        test_dataset = QADataset(\n",
        "            test_df['Question Body'].values,\n",
        "            test_df['Answer Body'].values,\n",
        "            tokenizer\n",
        "        )\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "        # Train the model\n",
        "        train_model(model, train_dataloader, val_dataloader, device)\n",
        "\n",
        "        # Evaluate model\n",
        "        print(\"\\nEvaluating model on test set...\")\n",
        "        evaluation_scores = evaluate_model(model, test_dataloader, device)\n",
        "\n",
        "        # Save evaluation results\n",
        "        save_evaluation_results(evaluation_scores)\n",
        "\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), 'qa_model.pt')\n",
        "        tokenizer.save_pretrained('qa_model')\n",
        "\n",
        "        # Save preprocessing statistics\n",
        "        with open('preprocessing_stats.txt', 'w') as f:\n",
        "            f.write(\"Preprocessing Statistics\\n\")\n",
        "            f.write(\"======================\\n\\n\")\n",
        "            f.write(f\"Original dataset size: {len(df)}\\n\")\n",
        "            f.write(f\"Average question length: {df['Question Body'].str.len().mean():.2f} characters\\n\")\n",
        "            f.write(f\"Average answer length: {df['Answer Body'].str.len().mean():.2f} characters\\n\")\n",
        "            f.write(f\"\\nDataset splits:\\n\")\n",
        "            f.write(f\"Training set: {len(train_df)} samples\\n\")\n",
        "            f.write(f\"Validation set: {len(val_df)} samples\\n\")\n",
        "            f.write(f\"Test set: {len(test_df)} samples\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}